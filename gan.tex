\section{How do GANs work?}

We have now seen several other generative models and explained that GANs do not
work in the same way that they do. But how do GANs themselves work?

\subsection{The GAN framework}

The basic idea of GANs is to set up a game between two players.
One of them is called the \newterm{generator}.
The generator creates samples that are intended to come from the
same distribution as the training data.
The other player is the \newterm{discriminator}.
The discriminator examines samples to determine whether they are real
or fake.
The discriminator learns using traditional supervised learning techniques,
dividing inputs into two classes (real or fake).
The generator is trained to fool the discriminator.
We can think of the generator as being like a counterfeiter, trying to
make fake money, and the discriminator as being like police, trying to
allow legitimate money and catch counterfeit money.
To succeed in this game, the counterfeiter must learn to make money that
is indistinguishable from genuine money, and the generator network must
learn to create samples that are drawn from the same distribution as the
training data.
The process is illustrated in \figref{fig:framework}.

\begin{figure}
\includegraphics[width=\textwidth]{framework}
\caption{The GAN framework pits two adversaries against each other in a game.
Each player is represented by a differentiable function controlled by a set
of parameters.
Typically these functions are implemented as deep neural networks.
The game plays out in two scenarios.
In one scenario, training examples $\vx$ are randomly sampled from the training
set and used as input for the first player, the discriminator, represented
by the function $D$. The goal of the discriminator is to output the probability
that its input is real rather than fake, under the assumption that half of the
inputs it is ever shown are real and half are fake.
In this first scenario, the goal of the discriminator is for $D(\vx)$ to be
near 1.
In the second scenario, inputs $\vz$ to the generator are randomly sampled from
the model's prior over the latent variables.
The discriminator then receives input $G(\vz)$, a fake sample created by the
generator.
In this scenario, both players participate. The discriminator strives to make
$D(G(z))$ approach 0 while the generative strives to make the same quantity
approach 1.
If both models have sufficient capacity, then the Nash equilibrium of this game
corresponds to the $G(\vz)$ being drawn from the same distribution as the training
data, and $D(\vx) = \frac{1}{2}$ for all $\vx$.
}
\label{fig:framework}
\end{figure}

Formally, GANs are a structured probabilistic model (see chapter 16 of
\citet{Goodfellow-et-al-2016} for an introduction to structured probabilistic
models) containing latent variables $\vz$ and observed variables $\vx$.
The graph structure is shown in \figref{fig:graph}.

The two players in the game are represented by two functions, each of which
is differentiable both with respect to its inputs and with respect to its
parameters.
The discriminator is a function $D$ that takes $\vx$ as input and uses
$\vtheta^{(D)}$ as parameters.
The generator is defined by a function $G$ that takes $\vz$ as input and
uses $\vtheta^{(G)}$ as parameters.

Both players have cost functions that are defined in terms of both players'
parameters.
The discriminator wishes to minimize $J^{(D)}\left( \vtheta^{(D)}, \vtheta^{(G)} \right)$
and must do so while controlling only $\vtheta^{(D)}$.
The generator wishes to minimize $J^{(G)}\left( \vtheta^{(D)}, \vtheta^{(G)} \right)$
and must do so while controlling only $\vtheta^{(G)}$.
Because each player's cost depends on the other player's parameters, but each player
cannot control the other player's parameters, this scenario is most
straightforward to describe as a game rather than as an optimization problem.
The solution to an optimization problem is a (local) minimum, a point in parameter space
where all neighboring points have greater or equal cost.
The solution to a game is a Nash equilibrium.
Here, we use the terminology of local differential Nash equilibria \citep{ratliff2013characterization}.
In this context, a Nash equilibrium is a tuple $(\vtheta^{(D)}, \vtheta^{(G)} )$
that is a local minimum of $J^{(D)}$ with respect to $\vtheta^{(D)}$ and a local
minimum of $J^{(G)}$ with respect to $\vtheta^{(G)}$.

\paragraph{The generator}
The generator is simply a differentiable function $G$.
When $\vz$ is sampled from some simple prior distribution,
$G(\vz)$ yields a sample of $\vx$ drawn from $\pmodel$.
Typically, a deep neural network is used to represent $G$.
Note that the inputs to the function $G$ do not need to correspond
to inputs to the first layer of the deep neural net; inputs may
be provided at any point throughout the network.
For example, we can partition $\vz$ into two vectors $\vz^{(1)}$
and $\vz^{(2)}$, then feed $\vz^{(1)}$ as input to the first layer
of the neural net and add $\vz^{(2)}$ to the last layer of the neural
net. If $\vz^{(2)}$ is Gaussian, this makes $\vx$ conditionally Gaussian
given $\vz^{(1)}$.
Another popular strategy is to apply additive or multiplicative noise to
hidden layers or concatenate noise to hidden layers of the neural net.
Overall, we see that there are very few restrictions on the design of
the generator net.
If we want $\pmodel$ to have full support on $\vx$ space we need the dimension
of $\vz$ to be at least as large as the dimension of $\vx$, and $G$ must be
differentiable, but those are the only requirements.
In particular, note that any model that can be trained with the nonlinear
ICA approach can be a GAN generator network.
The relationship with variational autoencoders is more complicated;
the GAN framework can train some models that the VAE framework cannot and vice
versa, but the two frameworks also have a large intersection.
The most salient difference is that, if relying on standard backprop,
VAEs cannot have discrete variables at the input to the generator,
while GANs cannot have discrete variables at the output of the generator.

\paragraph{The training process}
The training process consists of simultaneous SGD.
On each step, two minibatches are sampled: a minibatch of $\vx$ values from
the dataset and a minibatch of $\vz$ values drawn from the model's prior over
latent variables.
Then two gradient steps are made simultaneously:
one updating $\vtheta^{(D)}$ to reduce $J^{(D)}$
and one updating $\vtheta^{(G)}$ to reduce $J^{(G)}$.
In both cases, it is possible to use the gradient-based optimization algorithm
of your choice.
Adam \citep{kingma2014adam} is usually a good choice.
Many authors recommend running more steps of one player than the other, but
as of late 2016, the author's opinion is that the protocol that works the best
in practice is simultaneous gradient descent, with one step for each player.


\begin{figure}
  \centering
  \includegraphics{graph}
  \caption{The graphical model structure of GANs, which is also shared with
    VAEs, sparse coding, etc.
    It is directed graphical model where every latent variable influences
    every observed variable.
    Some GAN variants remove some of these connections.
  }
  \label{fig:graph}
\end{figure}

\subsection{Cost functions}

Several different cost functions may be used within the GANs framework.

\subsubsection{The discriminator's cost, $J^{(D)}$}

All of the different games designed for GANs so far use the same cost for the
discriminator, $J^{(D)}$. They differ only in terms of the cost used for the
generator, $J^{(G)}$.

The cost used for the discriminator is:
\begin{equation}
  J^{(D)}(\vtheta^{(D)}, \vtheta^{(G)}) = -\frac{1}{2} \E_{\vx \sim \pdata} \log D(\vx) - \frac{1}{2} \E_{\vz} \log \left(1 - D\left( G(z)\right) \right).
  \label{eq:discriminator_cost}
\end{equation}

This is just the standard cross-entropy cost that is minimized when training a standard binary classifier
with a sigmoid output.
The only difference is that the classifier is trained on two minibatches of data; one coming from the
dataset, where the label is $1$ for all examples, and one coming from the generator, where the label
is $0$ for all examples.

All versions of the GAN game encourage the discriminator to minimize \eqref{eq:discriminator_cost}.
In all cases, the discriminator has the same optimal strategy.
The reader is now encouraged to complete the exercise in \secref{sec:opt_d} and review its solution
given in \secref{sec:opt_d_soln}. This exercise shows how to derive the optimal discriminator strategy
and discusses the importance of the form of this solution.

We see that by training the discriminator, we are able to obtain an estimate of the ratio
\[
  \frac{\pdata(\vx)}{\pmodel(\vx)}
\]
at every point $\vx$.
Estimating this ratio enables us to compute a wide variety of divergences and their gradients.
This is the key approximation technique that sets GANs apart from variational autoencoders
and Boltzmann machines.
Other deep generative models make approximations based on lower bounds or Markov chains;
GANs make approximations based on using supervised learning to estimate a ratio of two densities.


\subsubsection{Minimax}

So far we have specified the cost function for only the discriminator.
A complete specification of the game requires that we specify a cost function also
for the generator.

The simplest version of the game is a \newterm{zero-sum game}, in which the sum of all player's
costs is always zero.
In this version of the game,
\begin{equation}
J^{(G)} = - J^{(D)}.
\label{eq:minimax}
\end{equation}

Because $J^{(G)}$ is tied directly to $J^{(D)}$, we can summarize the entire game with a
\newterm{value function} specifying the discriminator's payoff:
\[ V\left(\vtheta^{(D)}, \vtheta^{(G)} \right) = - J^{(D)} \left(\vtheta^{(D)}, \vtheta^{(G)} \right).\]

Zero-sum games are also called \newterm{minimax} games because their solution involves minimization
in an outer loop and maximization in an inner loop:
\[ \vtheta^{(G)*} = \argmin_{\vtheta^{(G)}} \max_{\vtheta^{(D)}} V\left(\vtheta^{(D)}, \vtheta^{(G)} \right) . \]

The minimax game is mostly of interest because it is easily amenable to theoretical analysis.
\citet{Goodfellow-et-al-NIPS2014-small} used this variant of the GAN game to show that learning in
this game resembles minimizing the Jensen-Shannon divergence between the data and the model distribution,
and that the game converges to its equilibrium if both players' policies can be updated directly in
function space.
In practice, the players are represented with deep neural nets and updates are made in parameter space,
so these results, which depend on convexity, do not apply.

\subsubsection{Heuristic, non-saturating game}

The cost used for the generator in the minimax game (\eqref{eq:minimax}) is useful for theoretical analysis,
but does not perform especially well in practice.

Minimizing the cross-entropy between a target class and a classifier's predicted distribution
is highly effective because the cost never saturates when the classifier has the wrong output.
The cost does eventually saturate, approaching zero, but only when the classifier has already
chosen the correct class.

In the minimax game, the discriminator minimizes a cross-entropy, but the generator maximizes
the same cross-entropy.
This is unfortunate for the generator, because when the discriminator successfully rejects
generator samples with high confidence, the generator's gradient vanishes.

To solve this problem, one approach is to continue to use cross-entropy minimization for the
generator.
Instead of flipping the sign on the discriminator's cost to obtain a cost for the generator,
we flip the target used to construct the cross-entropy cost.
The cost for the generator then becomes:
\[
  J^{(G)} = -\frac{1}{2} \E_\vz \log D(G(\vz))
\]

In the minimax game, the generator minimizes the log-probability of the discriminator being correct.
In this game, the generator maximizes the log-probability of the discriminator being mistaken.

This version of the game is heuristically motivated, rather than being motivated by a theoretical
concern.
The sole motivation for this version of the game is to ensure that each player has a strong
gradient when that player is ``losing'' the game.

In this version of the game, the game is no longer zero-sum, and cannot be described with a single
value function.

\subsection{The DCGAN architecture}

Most GANs today are at least loosely based on the DCGAN architecture \citep{radford2015unsupervised}.
DCGAN stands for ``deep, convolution GAN.'' Though GANs were both deep and convolutional prior to
DCGANs, the name DCGAN is useful to refer to this specific style of architecture.
Some of the key insights of the DCGAN architecture were to:
\begin{itemize}
  \item Use batch normalization layers in most layers of both the discriminator and the generator,
        with the two minibatches for the discriminator normalized separately.
        The last layer of the generator and first layer of the discriminator are not batch normalized,
        so that the model can learn the correct mean and scale of the data distribution.
        See \figref{fig:dcgan}.
  \item The overall network structure is mostly borrowed from the all-convolutional net \citep{Springenberg2015}.
        This architecture contains neither pooling nor ``unpooling'' layers.
        When the generator needs to increase the spatial dimension of the representation
        it uses transposed convolution with a stride greater than 1.
  \item The use of the Adam optimizer rather than SGD with momentum.
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{dcgan}
  \caption{The generator network used by a DCGAN. Figure reproduced from \citet{radford2015unsupervised}.}
\label{fig:dcgan}
\end{figure}

Prior to DCGANs, LAPGANs \citep{denton2015deep} were the only version of GAN
that had been able to scale to high resolution images.
LAPGANs require a multi-stage generation process in which multiple GANs
generate different levels of detail in a Laplacian pyramid representation
of an image.
DCGANs were the first GAN model to learn to generate high resolution images
in a single shot.
As shown in \figref{fig:dcgan_lsun}, DCGANs are able to generate high quality
images when trained on restricted domains of images, such as images of bedrooms.
DCGANs also clearly demonstrated that GANs learn to use their latent code
in meaningful ways, with simple arithmetic operations in latent space
having clear interpretation as arithmetic operations on semantic attributes
of the input, as demonstrated in \figref{fig:dcgan_face_arithmetic}.


\begin{figure}
\centering
$
\vcenter{
    \hbox{%
\includegraphics[width=.15\figwidth]{man_with_glasses.png} %
    }
}
\vcenter{\hbox{-}} %
\vcenter{\hbox{
\includegraphics[width=.15\figwidth]{man_without_glasses.png} %
}}
\vcenter{\hbox{+}} %
\vcenter{\hbox{
\includegraphics[width=.15\figwidth]{woman_without_glasses.png} %
}}
\vcenter{\hbox{=}} %
\vcenter{\hbox{
\includegraphics[width=.45\figwidth]{woman_with_glasses.png}
}
}
$
\caption{DCGANs demonstrated that GANs can learn a distributed representation
that disentangles the concept of gender from the concept of wearing
glasses. If we begin with the representation of the concept of a
man with glasses, then subtract the vector representing the concept
of a man without glasses, and finally add the vector representing
the concept of a woman without glasses, we obtain the vector representing
the concept of a woman with glasses. The generative model correctly
decodes all of these representation vectors to images that may be
recognized as belonging to the correct class.
Images reproduced from \citet{radford2015unsupervised}.
}
\label{fig:dcgan_face_arithmetic}
\end{figure}
